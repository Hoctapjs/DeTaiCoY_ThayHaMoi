import numpy as np
from sklearn.cluster import KMeans
from skimage import io, color
import time
import os
import pandas as pd
from datetime import datetime
import logging
from scipy.sparse import diags, csr_matrix
from joblib import Parallel, delayed
from numba import jit, prange

# H√†m t√≠nh ma tr·∫≠n RBF song song v·ªõi ƒë·∫ßu ra l√† ma tr·∫≠n th∆∞a
def compute_rbf_matrix_parallel(X, gamma, threshold=0.005, chunk_size=100):  # Gi·∫£m threshold
    """T√≠nh ma tr·∫≠n RBF Kernel song song v·ªõi ƒë·∫ßu ra l√† ma tr·∫≠n th∆∞a."""
    n, d = X.shape
    W_data = []
    W_row = []
    W_col = []
    
    def compute_chunk(i_start, i_end):
        local_data = []
        local_row = []
        local_col = []
        for i in range(i_start, i_end):
            for j in range(n):
                dist = np.sum((X[i] - X[j]) ** 2)
                value = np.exp(-gamma * dist)
                if value > threshold:
                    local_data.append(value)
                    local_row.append(i)
                    local_col.append(j)
        return local_data, local_row, local_col
    
    chunks = [(i, min(i + chunk_size, n)) for i in range(0, n, chunk_size)]
    results = Parallel(n_jobs=-1)(delayed(compute_chunk)(i, j) for i, j in chunks)
    
    # Gh√©p c√°c k·∫øt qu·∫£ t·ª´ t·ª´ng ph·∫ßn l·∫°i
    for data, row, col in results:
        W_data.extend(data)
        W_row.extend(row)
        W_col.extend(col)
    
    W = csr_matrix((W_data, (W_row, W_col)), shape=(n, n))
    return W

# T√≠nh ma tr·∫≠n tr·ªçng s·ªë song song v·ªõi ƒë·∫ßu ra l√† ma tr·∫≠n th∆∞a
def compute_weight_matrix(image, sigma_i=0.3, sigma_x=20.0, chunk_size=100):  # Tinh ch·ªânh sigma
    """T√≠nh ma tr·∫≠n tr·ªçng s·ªë song song v·ªõi ƒë·∫ßu ra l√† ma tr·∫≠n th∆∞a."""
    h, w, c = image.shape
    coords = np.array(np.meshgrid(np.arange(h), np.arange(w))).reshape(2, -1).T
    features = image.reshape(-1, c)
    
    gamma_i = 1 / (2 * sigma_i**2)
    gamma_x = 1 / (2 * sigma_x**2)
    
    start_features = time.time()
    W_features = compute_rbf_matrix_parallel(features, gamma_i, threshold=0.005, chunk_size=chunk_size)
    end_features = time.time()
    W_features_time = end_features - start_features
    
    start_coords = time.time()
    W_coords = compute_rbf_matrix_parallel(coords, gamma_x, threshold=0.005, chunk_size=chunk_size)
    end_coords = time.time()
    W_coords_time = end_coords - start_coords
    
    # Nh√¢n hai ma tr·∫≠n th∆∞a
    W = W_features.multiply(W_coords)
    return W, W_features_time, W_coords_time

# T√≠nh ma tr·∫≠n Laplacian v·ªõi ma tr·∫≠n th∆∞a
def compute_laplacian(W):
    D_diag = W.sum(axis=1).A1
    D = diags(D_diag)
    L = D - W
    return L, D

# H√†m t√≠ch v√¥ h∆∞·ªõng v·ªõi Numba
@jit(nopython=True)
def handle_dot(a, b):
    result = 0.0
    for i in range(len(a)):
        result += a[i] * b[i]
    return result

# Nh√¢n ma tr·∫≠n-vector v·ªõi Numba (song song h√≥a cho ma tr·∫≠n th∆∞a)
@jit(nopython=True, parallel=True)
def matrix_vector_product(A_data, A_indices, A_indptr, v, n, num_threads=os.cpu_count()):
    result = np.zeros(n)
    chunk_size = n // num_threads
    for t in prange(num_threads):
        start = t * chunk_size
        end = (t + 1) * chunk_size if t < num_threads - 1 else n
        for i in range(start, end):
            for j in range(A_indptr[i], A_indptr[i + 1]):
                result[i] += A_data[j] * v[A_indices[j]]
    return result

# Thu·∫≠t to√°n Lanczos cho ma tr·∫≠n th∆∞a v·ªõi song song h√≥a c·∫£i ti·∫øn
@jit(nopython=True, parallel=True)
def lanczos_sparse(A_data, A_indices, A_indptr, v, m, n, num_threads=os.cpu_count()):
    V = np.zeros((m, n))
    T = np.zeros((m, m))
    V[0, :] = v / np.linalg.norm(v)
    
    w = matrix_vector_product(A_data, A_indices, A_indptr, V[0, :], n)
    alpha = handle_dot(w, V[0, :])
    w = w - alpha * V[0, :]
    T[0, 0] = alpha
    
    # chunk_size = m // num_threads
    zero_vector = np.zeros(n, dtype=np.float64)  # Th√™m zero vector cho nh√°nh else
    for j in prange(1, m):  # Song song h√≥a v√≤ng l·∫∑p ( j ), nh∆∞ng v√¨ c√≥ break, hi·ªáu qu·∫£ song song b·ªã h·∫°n ch·∫ø (Numba kh√¥ng t·ªëi ∆∞u t·ªët v·ªõi v√≤ng l·∫∑p c√≥ tho√°t s·ªõm)

        beta = np.linalg.norm(w)
        if beta < 1e-10:
            break
        V[j, :] = w / beta
        # Tr·ª±c giao h√≥a (Gram-Schmidt) ch·∫∑t ch·∫Ω
        for i in range(j):
            proj = handle_dot(V[j, :], V[i, :])
            V[j, :] -= proj * V[i, :]
        norm_vj = np.linalg.norm(V[j, :])
        if norm_vj < 1e-10:
            break
        V[j, :] /= norm_vj
        
        w = matrix_vector_product(A_data, A_indices, A_indptr, V[j, :], n)
        alpha = handle_dot(w, V[j, :])
        w = w - alpha * V[j, :] - (beta * V[j-1, :] if j > 0 else zero_vector)  # S·ª≠ d·ª•ng zero_vector
    
        T[j, j] = alpha
        if j > 0:
            T[j-1, j] = beta
            T[j, j-1] = beta
    
    # ƒê·∫£m b·∫£o T ƒë·ªëi x·ª©ng
    T = (T + T.T) / 2
    return T, V

# T√≠nh tr·ªã ri√™ng v√† vector ri√™ng
def compute_eigen(L, D, k=2):
    D_diag = D.diagonal().copy()
    D_diag[D_diag < 1e-10] = 1e-10
    D_inv_sqrt = diags(1.0 / np.sqrt(D_diag))
    L_normalized = D_inv_sqrt @ L @ D_inv_sqrt
    
    v0 = np.random.rand(L.shape[0])
    v0 /= np.linalg.norm(v0)
    
    n = L.shape[0]
    L_csr = L_normalized.tocsr()
    start_lanczos = time.time()
    T, V = lanczos_sparse(L_csr.data, L_csr.indices, L_csr.indptr, v0, m=k + 50, n=n)
    end_lanczos = time.time()
    
    lanczos_time = end_lanczos - start_lanczos
    logging.info(f"Th·ªùi gian Lanczos: {lanczos_time:.6f} gi√¢y, n={n}")
    
    # L·∫•y k√≠ch th∆∞·ªõc th·ª±c t·∫ø c·ªßa T
    j = T.shape[0] - 1  # S·ªë l·∫ßn l·∫∑p th·ª±c t·∫ø
    if j < k:
        raise ValueError(f"S·ªë l·∫ßn l·∫∑p th·ª±c t·∫ø ({j}) nh·ªè h∆°n k ({k})")
    
    # T√≠nh tr·ªã ri√™ng v√† vector ri√™ng c·ªßa ma tr·∫≠n T
    eigvals, eigvecs_T = np.linalg.eig(T)
    idx = np.argsort(eigvals)
    eigvals = eigvals[idx]
    eigvecs_T = eigvecs_T[:, idx]
    
    # B·ªè vector ri√™ng nh·ªè nh·∫•t (tr·ªã ri√™ng g·∫ßn 0) v√† l·∫•y k vector
    eigvecs_T = eigvecs_T[:, 1:k+1]
    eigvecs_original = D_inv_sqrt @ (V[:j+1, :].T @ eigvecs_T)
    
    # Chu·∫©n h√≥a vector ri√™ng k·ªπ h∆°n
    for i in range(eigvecs_original.shape[1]):
        norm = np.linalg.norm(eigvecs_original[:, i])
        if norm > 1e-10:
            eigvecs_original[:, i] /= norm
        else:
            eigvecs_original[:, i] = np.zeros_like(eigvecs_original[:, i])
    
    logging.info(f"Tr·ªã ri√™ng: {eigvals[:k+1]}")
    logging.info(f"Vector ri√™ng (m·∫´u): {eigvecs_original[:5, :]}")
    return eigvecs_original, lanczos_time

# G√°n nh√£n
def assign_labels(eigen_vectors, k):
    return KMeans(n_clusters=k, random_state=42, n_init=50).fit(eigen_vectors).labels_  # TƒÉng n_init

# H√†m t√°ch ·∫£nh th√†nh 4 ph·∫ßn
def split_image(image):
    h, w, c = image.shape
    h_half, w_half = h // 2, w // 2
    parts = [
        image[0:h_half, 0:w_half, :],        # Top-left (0-24, 0-24)
        image[0:h_half, w_half:w, :],        # Top-right (0-24, 25-49)
        image[h_half:h, 0:w_half, :],       # Bottom-left (25-49, 0-24)
        image[h_half:h, w_half:w, :]        # Bottom-right (25-49, 25-49)
    ]
    return parts


# L∆∞u file .seg
def save_seg_file(labels, image_shape, output_path, image_name="image"):
    h, w = image_shape[:2]
    labels_2d = labels.reshape(h, w)
    unique_labels = np.unique(labels)
    segments = len(unique_labels)

    header = [
        "format ascii cr",
        f"date {datetime.now().strftime('%a %b %d %H:%M:%S %Y')}",
        f"image {image_name}",
        "user 1102",
        f"width {w}",
        f"height {h}",
        f"segments {segments}",
        "gray 0",
        "invert 0",
        "flipflop 0",
        "data"
    ]

    data_lines = []
    for row in range(h):
        row_labels = labels_2d[row, :]
        start_col = 0
        current_label = row_labels[0]

        for col in range(1, w):
            if row_labels[col] != current_label:
                data_lines.append(f"{int(current_label)} {row} {start_col} {col - 1}")
                start_col = col
                current_label = row_labels[col]
        data_lines.append(f"{int(current_label)} {row} {start_col} {w - 1}")

    with open(output_path, "w", encoding="utf-8") as f:
        f.write("\n".join(header) + "\n")
        f.write("\n".join(data_lines) + "\n")
    print(f"‚úÖ File SEG ƒë√£ l∆∞u: {output_path}")

# H√†m ch√≠nh x·ª≠ l√Ω ·∫£nh v·ªõi t√°ch th√†nh 4 ph·∫ßn
def normalized_cuts(lan, imagename, image_path, output_path):
    start_cpu = time.time()
    image = io.imread(image_path)
    image = color.gray2rgb(image) if image.ndim == 2 else image[:, :, :3] if image.shape[2] == 4 else image
    image = image / 255.0

    # T√°ch ·∫£nh th√†nh 4 ph·∫ßn
    image_parts = split_image(image)
    part_names = ["top_left", "top_right", "bottom_left", "bottom_right"]
    total_time_parts = []
    wf_times = []
    wc_times = []
    w_all_times = []
    lanczos_times = []

    for idx, (part, part_name) in enumerate(zip(image_parts, part_names)):
        k = 2  # S·ªë ph√¢n v√πng

        # T√≠nh to√°n ma tr·∫≠n tr·ªçng s·ªë v√† ph√¢n ƒëo·∫°n cho t·ª´ng ph·∫ßn
        W, W_f, W_c = compute_weight_matrix(part, sigma_i=0.3, sigma_x=20.0)
        W_all = W_f + W_c
        L, D = compute_laplacian(W)
        vecs, lanczos_time = compute_eigen(L, D, k)
        labels = assign_labels(vecs, k)

        # L∆∞u file .seg cho t·ª´ng ph·∫ßn
        seg_output_path = f"{imagename}_segmentation_{lan}_{part_name}.seg"
        save_seg_file(labels, part.shape, seg_output_path, f"{imagename}_{part_name}")

        # Ghi l·∫°i th·ªùi gian
        end_part = time.time()
        total_time_parts.append(end_part - start_cpu)
        wf_times.append(W_f)
        wc_times.append(W_c)
        w_all_times.append(W_all)
        lanczos_times.append(lanczos_time)

    end_cpu = time.time()
    total_cpu_time = end_cpu - start_cpu
    return total_cpu_time, wf_times, wc_times, w_all_times, lanczos_times

# H√†m ch·∫°y nhi·ªÅu ·∫£nh v√† l∆∞u k·∫øt qu·∫£
def kiemThuChayNhieuLan(i, name, folder_path, output_excel_base="results"):
    if not os.path.isdir(folder_path):
        print(f"‚ùå Th∆∞ m·ª•c {folder_path} kh√¥ng t·ªìn t·∫°i!")
        return

    image_files = [f for f in os.listdir(folder_path) if f.lower().endswith(('.png', '.jpg', '.jpeg', '.bmp', '.gif'))]
    if not image_files:
        print(f"‚ùå Kh√¥ng t√¨m th·∫•y file ·∫£nh n√†o trong {folder_path}!")
        return

    results = []
    for idx, file_name in enumerate(image_files, start=1):
        image_path = os.path.join(folder_path, file_name)
        print(f"üì∑ ƒêang x·ª≠ l√Ω ·∫£nh {idx}: {image_path}")

        imagename = os.path.splitext(file_name)[0]
        total_time, wf_times, wc_times, w_all_times, lanczos_times = normalized_cuts(i, imagename, image_path, output_excel_base)

        # Ghi k·∫øt qu·∫£ cho t·ª´ng ph·∫ßn
        part_names = ["top_left", "top_right", "bottom_left", "bottom_right"]
        for part_idx, part_name in enumerate(part_names):
            results.append([
                i, idx, f"{file_name}_{part_name}", 
                wf_times[part_idx], wc_times[part_idx], w_all_times[part_idx], lanczos_times[part_idx]
            ])

    df = pd.DataFrame(results, columns=[
        "L·∫ßn ch·∫°y", "·∫¢nh s·ªë", "T√™n ·∫£nh", 
        "Th·ªùi gian W ƒë·∫∑c tr∆∞ng (s)", "Th·ªùi gian W t·ªça ƒë·ªô (s)", "Th·ªùi gian W All", "Th·ªùi gian Lanczos (s)"
    ])
    output_excel = f"{output_excel_base}_{name}_{i}.xlsx"
    df.to_excel(output_excel, index=False, engine='openpyxl')
    print(f"‚úÖ K·∫øt qu·∫£ Excel ƒë√£ l∆∞u v√†o {output_excel}")